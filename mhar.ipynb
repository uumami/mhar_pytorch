{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mhar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFm08dsy4s8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "ceb95975-fa24-47a2-a010-ac70911ce2da"
      },
      "source": [
        "'''\n",
        "Avoid turning off your notebook or timeout sessions using this:\n",
        " \n",
        "Mozilla:\n",
        "  type:\n",
        "  >ctrl + shift + i\n",
        " \n",
        "  Go to console \n",
        " \n",
        "  Paste and run this:\n",
        "  >>\n",
        "  function ClickConnect(){\n",
        "  console.log(\"Working\"); \n",
        "  document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "  }setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAvoid turning off your notebook or timeout sessions using this:\\n \\nMozilla:\\n  type:\\n  >ctrl + shift + i\\n \\n  Go to console \\n \\n  Paste and run this:\\n  >>\\n  function ClickConnect(){\\n  console.log(\"Working\"); \\n  document.querySelector(\"colab-toolbar-button#connect\").click() \\n  }setInterval(ClickConnect,60000)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1WTC_cX4VB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save points you sampled to your drive\n",
        "path_save_samples = '/drive/My Drive/mhar_paper_samples_2/'\n",
        "# Save samples times to your drive\n",
        "path_save_times = '/drive/My Drive/mhar_test_times_2/'\n",
        "path_save_times_p100 = '/drive/My Drive/mhar_test_times_p100/'\n",
        " \n",
        "# Run times?\n",
        "run_times = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojmnjrF-K0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "913394f6-56c0-4e22-d4a8-a8b9d4d81272"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul  6 08:15:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDZmyxJg4hiv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "ebf21914-cd0d-4861-d835-4edd1beada90"
      },
      "source": [
        "# Mount google drive\n",
        "gpu_ = 'p100'\n",
        "from google.colab import drive\n",
        "drive.mount('/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEGyPVQnuWhf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9176e2e5-7a11-40ad-a99d-d125703a79e3"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdrhXRt6E1Tv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "f9f08dd5-0edd-4d25-ee5b-5fa69f89102e"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MemTotal:       13333556 kB\n",
            "MemFree:        10439648 kB\n",
            "MemAvailable:   12439744 kB\n",
            "Buffers:           76124 kB\n",
            "Cached:          2079720 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           765088 kB\n",
            "Inactive:        1841756 kB\n",
            "Active(anon):     429260 kB\n",
            "Inactive(anon):      332 kB\n",
            "Active(file):     335828 kB\n",
            "Inactive(file):  1841424 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:              7040 kB\n",
            "Writeback:            16 kB\n",
            "AnonPages:        450944 kB\n",
            "Mapped:           245908 kB\n",
            "Shmem:               948 kB\n",
            "Slab:             167316 kB\n",
            "SReclaimable:     127468 kB\n",
            "SUnreclaim:        39848 kB\n",
            "KernelStack:        4176 kB\n",
            "PageTables:         5908 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6666776 kB\n",
            "Committed_AS:    2801860 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:              920 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      105660 kB\n",
            "DirectMap2M:     5136384 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDUXbk3L9Ofa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "580698aa-8928-4cb5-a160-0a4737fef6f5"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8mGM_Lk9kyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b644a4a6-19d6-44fd-a992-b3ef969da2fb"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2kQq09x9n3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4rU7n6W-Faq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYWkR_hm_Cz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "584b8935-9dec-4b97-f1b0-e80cb6355376"
      },
      "source": [
        "# Set float 64 as default precision, can be changed to 32\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2fQiFMwFwHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Auxiliary routines for the mhar, projection, random numbers, etc\n",
        "'''\n",
        "\n",
        "# Create Projection Matrix\n",
        "def create_projection_matrix(ae, check=True, device='cpu'):\n",
        "  '''\n",
        "  ae : tensor with the equality matrix, must be a matrix\n",
        "  check: Boolean for evaluating numerical estability of the inversion\n",
        "  '''\n",
        "  aet = torch.transpose(ae,0,1)\n",
        "  ae_aux = torch.matmul(ae, aet)\n",
        "  ae_inv = torch.inverse(ae_aux)\n",
        "  # Check numerycal estability\n",
        "  if check:\n",
        "    est = torch.mm(ae_inv, ae_aux)\n",
        "    est = torch.max(torch.abs(est - torch.eye(est.shape[0], device=device)))\n",
        "    print('Max non zero error: ', est)\n",
        "    del est\n",
        "\n",
        "  la = torch.matmul(aet, ae_inv)\n",
        "  projection_matrix = torch.matmul(la,ae)\n",
        "  projection_matrix = torch.eye(projection_matrix.shape[0], device=device) - projection_matrix\n",
        "  \n",
        "\n",
        "  # Free Memory\n",
        "  del ae_aux\n",
        "  del ae_inv\n",
        "  del la\n",
        "\n",
        "  return projection_matrix\n",
        "\n",
        "\n",
        "\n",
        "# Draw random numbers\n",
        "def create_h(n,z, generator,device='cpu'):\n",
        "\n",
        "  if '64' in str(torch.get_default_dtype()):\n",
        "    if device != 'cpu':\n",
        "      h = torch.cuda.DoubleTensor(z,n,1).normal_(generator=generator)\n",
        "    else:\n",
        "      h = torch.DoubleTensor(z, n,1).normal_(generator=generator)\n",
        "  else:\n",
        "    if device != 'cpu':\n",
        "      h = torch.cuda.DoubleTensor(z,n,1).normal_(generator=generator)\n",
        "    else:\n",
        "      h = torch.DoubleTensor(z,n,1).normal_(generator=generator)\n",
        "  return h\n",
        "\n",
        " # Draw random numbers\n",
        "def draw_uniform(z, generator,device='cpu'):\n",
        "\n",
        "  # if 64 \n",
        "  if '64' in str(torch.get_default_dtype()):\n",
        "    if device != 'cpu':\n",
        "      h = torch.cuda.DoubleTensor(z,1).uniform_(generator=generator)\n",
        "    else:\n",
        "      h = torch.DoubleTensor(z,1).uniform_(generator=generator)\n",
        "  else:\n",
        "    if device != 'cpu':\n",
        "      h = torch.cuda.FloatTensor(z,1).uniform_(generator=generator)\n",
        "    else:\n",
        "      h = torch.FloatTensor(z,1).uniform_(generator=generator)\n",
        "  return h \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n19h3TbRtq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MHAR\n",
        "def mhar(z, \n",
        "         ai=torch.empty(0), bi=torch.empty(0), \n",
        "         ae=torch.empty(0), be=torch.empty(0), \n",
        "         x_0=torch.empty(0), \n",
        "         T=1,seed=None, thinning = None,\n",
        "         check=True, device='cpu', save=True):\n",
        "  \n",
        "  print(device)\n",
        "\n",
        "  ##### Set min and max values\n",
        "  min_ = torch.finfo(torch.get_default_dtype()).min + 2.0\n",
        "  max_ = torch.finfo(torch.get_default_dtype()).max - 2.0\n",
        "  eps_ = torch.finfo(torch.get_default_dtype()).eps\n",
        "  tiny_ = torch.finfo(torch.get_default_dtype()).tiny\n",
        "\n",
        "  #### Set seed\n",
        "  random_gen = torch.Generator(device=device)\n",
        "  if seed:\n",
        "    random_gen.manual_seed(seed)\n",
        "  else:\n",
        "    random_gen.seed()\n",
        "\n",
        "  #### Get dimensions\n",
        "  n = ai.shape[1]\n",
        "  mi = ai.shape[0]\n",
        "  me = ae.shape[0]\n",
        "  print('n: ', n, '  mi:', mi, '  me:', me, '  z:', z)\n",
        "  # Check if the polytope is full dimensional\n",
        "  non_full = me > 0\n",
        "\n",
        "  #### Set thinning factor\n",
        "  if thinning:\n",
        "    pass\n",
        "  else:\n",
        "    thinning = int((n-1)*(n-1)*(n-1))\n",
        "  print('Thinning factor:', thinning)\n",
        "\n",
        "\n",
        "\n",
        "  #### Copy tensors to gpu\n",
        "  if 'cpu' not in str(device):\n",
        "    ai = ai.cuda()\n",
        "    bi = bi.cuda()\n",
        "    x = x_0.cuda()\n",
        "    if non_full:\n",
        "      ae = ae.cuda()\n",
        "      be = be.cuda()\n",
        "  else:\n",
        "    x = x_0\n",
        "\n",
        "  ####Create projection matrix\n",
        "  if non_full:\n",
        "    pr = create_projection_matrix(ae, check, device)\n",
        "\n",
        "  #### Pad Matrix\n",
        "  x = x.repeat(z, 1, 1)\n",
        "  \n",
        "\n",
        "  #################### Iteration Loop #######################\n",
        "  t = 1\n",
        "  burned = 0\n",
        "\n",
        "  while t<=T:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #### Draw direction\n",
        "    h = create_h(n,z, generator=random_gen,device=device)\n",
        "\n",
        "    #### Project\n",
        "    if non_full:\n",
        "      d = torch.matmul(pr,h)\n",
        "    else:\n",
        "      d = h\n",
        "    ##### B - AX / AD\n",
        "    numerator = bi - torch.matmul(ai,x)\n",
        "    denominator = torch.matmul(ai,d)\n",
        "    # Overwrite nuemrator to keep memory free\n",
        "    numerator = numerator/denominator\n",
        "    # From the positive denominators you want the smallest one\n",
        "    lambda_pos = torch.min(~(denominator > 0.0)*max_ + \n",
        "                           (denominator > 0.0)*numerator,1).values\n",
        "    # From the negative denominators you want the biggest\n",
        "    lambda_neg = torch.max(~(denominator < 0.0)*min_ + \n",
        "                           (denominator < 0.0)*numerator,1).values\n",
        "    #### Uniform draw\n",
        "    u = draw_uniform(z, generator=random_gen,device=device)\n",
        "    theta = (1.0- u)*lambda_pos + (u)*lambda_neg \n",
        "    \n",
        "    ##### New X\n",
        "    x = x + d*theta[:,None]\n",
        "\n",
        "    ##### Manage the burning rate and save points\n",
        "    if burned >= thinning:\n",
        "      print(t)\n",
        "\n",
        "      if (t == 1) & (save):\n",
        "        X = torch.flatten(x, start_dim=1)\n",
        "        X = X.cpu()\n",
        "      if (t > 1) & (save):\n",
        "        X = torch.cat((X, \n",
        "                       torch.flatten(x, start_dim=1).cpu()), \n",
        "                      dim=0)\n",
        "      t = t + 1\n",
        "      burned = -1\n",
        "\n",
        "    burned = burned + 1\n",
        "\n",
        "  if save:\n",
        "    return X\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXbfhRmig52-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Hypercube Creation\n",
        "'''\n",
        "import time\n",
        "\n",
        "def create_hyper(n):\n",
        "  # Create Inequalities\n",
        "  hyper_ai = torch.cat((torch.eye(n),torch.eye(n)*-1.0), dim=0)\n",
        "  hyper_bi = torch.from_numpy(np.array([1]*2*n))[:,None]\n",
        "\n",
        "  return hyper_ai, hyper_bi\n",
        "\n",
        "def create_inner_point_hyper(n):\n",
        "  x_0 = torch.empty(n, 1, dtype=torch.float64)\n",
        "  x_0.fill_(0.01)\n",
        "  return x_0\n",
        "\n",
        "def debug_hyper(n, z=1, thinning=100, device='cpu', save=True, T=1, seed=123):\n",
        " \n",
        "  hyper_ai, hyper_bi = create_hyper(n)\n",
        "  x_0 = create_inner_point_hyper(n)\n",
        "\n",
        "  start_time = time.time()\n",
        "  X = mhar(z = z,\n",
        "       ai= hyper_ai, \n",
        "       bi = hyper_bi,\n",
        "       x_0 = x_0, \n",
        "       seed=seed, \n",
        "       check=True,\n",
        "       T = T,\n",
        "       thinning=thinning,\n",
        "       device=device,\n",
        "       save=save)\n",
        "  time_=  (time.time() - start_time)\n",
        "\n",
        "  print('End')\n",
        "  return X, time_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIp8yFeP-uuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Simplex Creation\n",
        "'''\n",
        "def create_simplex(n):\n",
        "  # Create Inequalities\n",
        "  simplex_ai = torch.eye(n)*-1.0\n",
        "  simplex_bi = torch.empty(n, 1, dtype=torch.float64)\n",
        "  simplex_bi.fill_(0.0)\n",
        "\n",
        "  # Create Equalities\n",
        "  simplex_ae = torch.empty(1, n, dtype=torch.float64)\n",
        "  simplex_ae.fill_(1.0)\n",
        "  simplex_be = torch.empty(1, 1, dtype=torch.float64)\n",
        "  simplex_be.fill_(1.0)\n",
        "  return simplex_ai, simplex_bi, simplex_ae, simplex_be\n",
        "\n",
        "def create_inner_point_simplex(n):\n",
        "  x_0 = torch.empty(n, 1, dtype=torch.float64)\n",
        "  x_0.fill_(1.0/n)\n",
        "  return x_0\n",
        "\n",
        "def debug_simplex(n, z=1,thinning=100,device='cpu', save=True, T=1, seed=123):\n",
        " \n",
        "  simplex_ai, simplex_bi, simplex_ae, simplex_be = create_simplex(n)\n",
        "  x_0 = create_inner_point_simplex(n)\n",
        "\n",
        "  start_time = time.time()\n",
        "  X = mhar(z = z,\n",
        "       ai= simplex_ai, \n",
        "       bi = simplex_bi,\n",
        "       ae = simplex_ae, \n",
        "       be = simplex_be, \n",
        "       x_0 = x_0, \n",
        "       seed=seed, \n",
        "       check=True,\n",
        "       T = T,\n",
        "       thinning=thinning,\n",
        "       device=device,\n",
        "       save=save)\n",
        "  print('End')\n",
        "  time_=  (time.time() - start_time)\n",
        "  return X, time_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW74b7y_lyTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# X, t = debug_hyper(n=10, z=100, thinning=100000, device='cuda', \n",
        "#                    T=1, save=True)\n",
        "\n",
        "# X = debug_simplex(n=1000, z=100, thinning=100000, device='cuda', T=1, save=False)\n",
        "# torch.sum(X, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBqEOPznBx0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "\n",
        "\n",
        "def run_time_sims(n_list = [],\n",
        "                  z_list = [],\n",
        "                  thinning_ = 30000,\n",
        "                  figure = 'simplex',\n",
        "                  save_pp = '/drive/My Drive/mhar_test_times_2/',\n",
        "                  postfix = '_data.csv'):\n",
        "  \n",
        "  drive.mount('/drive') \n",
        "  l_time_ = []\n",
        "  l_z_ = []\n",
        "  l_n_ = []\n",
        "\n",
        "  save_hash= randint(1,1000000000000)\n",
        "\n",
        "  for n_ in n_list:\n",
        "      for z_ in z_list:\n",
        "        start_time = time.time()\n",
        "\n",
        "        if figure == 'simplex':\n",
        "          X, time_ = debug_simplex(n=n_, z=z_,thinning=thinning_,device='cuda', \n",
        "                      save=False)\n",
        "        elif 'hypercube' == figure:\n",
        "          X, time_ = debug_hyper(n=n_, z=z_,thinning=thinning_,device='cuda',\n",
        "                      save=False)\n",
        "        \n",
        "        points_ = z_*thinning_\n",
        "        l_time_.append(time_)\n",
        "        l_z_.append(z_)\n",
        "        l_n_.append(n_)\n",
        "\n",
        "        print(\"--- %s seconds ---\" % (time_))\n",
        "        print(\"--- %s points per sec ---\" % (points_/time_))\n",
        "        performance = pd.DataFrame.from_dict({'n':l_n_, 'z':l_z_, \n",
        "                                              'time': l_time_})\n",
        "        performance['thi'] = thinning_ \n",
        "        performance['gpu'] = gpu_ \n",
        "        performance['figure'] = figure\n",
        "\n",
        "        performance['samples_sec'] = performance['z'] * thinning_ / performance['time']\n",
        "        name = save_pp + str(save_hash) + postfix\n",
        "        print(name)\n",
        "        performance.to_csv(name, index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWjS9rivCuk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_times:    \n",
        "  # Low dimensional Simplex\n",
        "\n",
        "  for it in range(1,11):\n",
        "    run_time_sims(n_list = [3, 5, 15, 25, 50],\n",
        "                      z_list = [1, 500, 1000, 2500, 5000, 10000],\n",
        "                      thinning_ = 30000,\n",
        "                      figure = 'simplex',\n",
        "                      save_pp = path_save_times,\n",
        "                      postfix = '_data.csv')\n",
        "    print()\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(it)\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print()\n",
        "\n",
        "\n",
        "  # Medium dimensional Simplex\n",
        "\n",
        "  for it in range(1,11):\n",
        "    run_time_sims(n_list = [100, 250, 300, 500, 1000],\n",
        "                      z_list = [1, 500, 1500, 3000, 4000],\n",
        "                      thinning_ = 30000,\n",
        "                      figure = 'simplex',\n",
        "                      save_pp = path_save_times,\n",
        "                      postfix = '_data.csv')\n",
        "    print()\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(it)\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print()\n",
        "\n",
        "  # HIgh dimensional Simplex\n",
        "\n",
        "  for it in range(1,11):\n",
        "    run_time_sims(n_list = [2500, 5000],\n",
        "                      z_list = [500, 1000],\n",
        "                      thinning_ = 10000,\n",
        "                      figure = 'simplex',\n",
        "                      save_pp = path_save_times,\n",
        "                      postfix = '_data.csv')\n",
        "    print()\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(it)\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkLqa3xOKzuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_times:  \n",
        "  # Low dimensional hypercube\n",
        "\n",
        "  for it in range(1,11):\n",
        "    run_time_sims(n_list = [3, 5, 15, 25, 50],\n",
        "                      z_list = [1, 500, 1000, 2500, 5000, 10000],\n",
        "                      thinning_ = 30000,\n",
        "                      figure = 'hypercube',\n",
        "                      save_pp = path_save_times,\n",
        "                      postfix = '_data.csv')\n",
        "    print()\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(it)\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print()\n",
        "\n",
        "    # Medium dimensional hypercube\n",
        "\n",
        "  for it in range(1,11):\n",
        "    run_time_sims(n_list = [100, 250, 500, 1000],\n",
        "                      z_list = [1, 500, 1500, 3000, 4000],\n",
        "                      thinning_ = 30000,\n",
        "                      figure = 'hypercube',\n",
        "                      save_pp = path_save_times,\n",
        "                      postfix = '_data.csv')\n",
        "    print()\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(it)\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print()\n",
        "\n",
        "  # HIgh dimensional hypercube\n",
        "\n",
        "  for it in range(1,11):\n",
        "    run_time_sims(n_list = [2500, 5000],\n",
        "                      z_list = [500, 1000, 1500],\n",
        "                      thinning_ = 10000,\n",
        "                      figure = 'hypercube',\n",
        "                      save_pp = path_save_times,\n",
        "                      postfix = '_data.csv')\n",
        "    print()\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(it)\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7ZYC3Z4DdbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "def run_point_sims(n_list = [],\n",
        "                  z_list = [],\n",
        "                  thinning_ = None,\n",
        "                  figure = 'simplex',\n",
        "                  save_pp = '/drive/My Drive/mhar_paper_samples/',\n",
        "                  T = 1,\n",
        "                   seed = 1):\n",
        "  \n",
        "  drive.mount('/drive') \n",
        "  l_time_ = []\n",
        "  l_z_ = []\n",
        "  l_n_ = []\n",
        "    \n",
        "  for n_ in n_list:\n",
        "    for z_ in z_list:\n",
        "\n",
        "      name = str(seed) + '_' + figure + '%' + str(z_) + '%' + str(n_) + '.csv'\n",
        "      # Check if the simulation already exists\n",
        "      onlyfiles = [f for f in listdir(save_pp) if isfile(join(save_pp, f))]\n",
        "      has_to_run = name not in onlyfiles\n",
        "\n",
        "      if has_to_run:\n",
        "        if figure == 'simplex':\n",
        "          if not None:\n",
        "            thinning_ = (n_ - 1)*(n_ - 1)*(n_ - 1)\n",
        "          X, time_ = debug_simplex(n=n_, z=z_,thinning=thinning_,device='cuda', \n",
        "                        save=True, seed=seed, T=T)\n",
        "        elif 'hypercube' == figure:\n",
        "          if not None:\n",
        "            thinning_ = (n_)*(n_)*(n_)         \n",
        "          X, time_ = debug_hyper(n=n_, z=z_,thinning=thinning_,device='cuda',\n",
        "                        save=True, seed=seed, T=T)\n",
        "          \n",
        "        path_save = save_pp + name # Save path\n",
        "        print(path_save)\n",
        "        X_data = pd.DataFrame(X.numpy())\n",
        "        X_data.to_csv(path_save, index=False)\n",
        "        print(name)\n",
        "      else:\n",
        "        print(name, ' existed already')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzFFZcXLicwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b0160dc-e236-40e0-d9ab-b246b743216f"
      },
      "source": [
        "# for it in range(1,11):\n",
        "for it in range(1,11):\n",
        "  run_point_sims(n_list = [3, 15, 25, 50],\n",
        "                    z_list = [1000],\n",
        "                    thinning_ = None,\n",
        "                    figure = 'hypercube',\n",
        "                    save_pp = path_save_samples,\n",
        "                    T = 10,\n",
        "                    seed = it * 133\n",
        "                )\n",
        "  run_point_sims(n_list = [3, 15, 25, 50],\n",
        "                    z_list = [1000],\n",
        "                    thinning_ = None,\n",
        "                    figure = 'simplex',\n",
        "                    save_pp = path_save_samples,\n",
        "                    T = 10,\n",
        "                    seed = it * 133\n",
        "                )\n",
        "  print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "  print(it)\n",
        "  print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "133_hypercube%1000%3.csv  existed already\n",
            "133_hypercube%1000%15.csv  existed already\n",
            "133_hypercube%1000%25.csv  existed already\n",
            "133_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "133_simplex%1000%3.csv  existed already\n",
            "133_simplex%1000%15.csv  existed already\n",
            "133_simplex%1000%25.csv  existed already\n",
            "133_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "1\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "266_hypercube%1000%3.csv  existed already\n",
            "266_hypercube%1000%15.csv  existed already\n",
            "266_hypercube%1000%25.csv  existed already\n",
            "266_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "266_simplex%1000%3.csv  existed already\n",
            "266_simplex%1000%15.csv  existed already\n",
            "266_simplex%1000%25.csv  existed already\n",
            "266_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "2\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "399_hypercube%1000%3.csv  existed already\n",
            "399_hypercube%1000%15.csv  existed already\n",
            "399_hypercube%1000%25.csv  existed already\n",
            "399_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "399_simplex%1000%3.csv  existed already\n",
            "399_simplex%1000%15.csv  existed already\n",
            "399_simplex%1000%25.csv  existed already\n",
            "399_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "3\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "532_hypercube%1000%3.csv  existed already\n",
            "532_hypercube%1000%15.csv  existed already\n",
            "532_hypercube%1000%25.csv  existed already\n",
            "532_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "532_simplex%1000%3.csv  existed already\n",
            "532_simplex%1000%15.csv  existed already\n",
            "532_simplex%1000%25.csv  existed already\n",
            "532_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "4\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "665_hypercube%1000%3.csv  existed already\n",
            "665_hypercube%1000%15.csv  existed already\n",
            "665_hypercube%1000%25.csv  existed already\n",
            "665_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "665_simplex%1000%3.csv  existed already\n",
            "665_simplex%1000%15.csv  existed already\n",
            "665_simplex%1000%25.csv  existed already\n",
            "665_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "5\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "798_hypercube%1000%3.csv  existed already\n",
            "798_hypercube%1000%15.csv  existed already\n",
            "798_hypercube%1000%25.csv  existed already\n",
            "798_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "798_simplex%1000%3.csv  existed already\n",
            "798_simplex%1000%15.csv  existed already\n",
            "798_simplex%1000%25.csv  existed already\n",
            "798_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "6\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "931_hypercube%1000%3.csv  existed already\n",
            "931_hypercube%1000%15.csv  existed already\n",
            "931_hypercube%1000%25.csv  existed already\n",
            "931_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "931_simplex%1000%3.csv  existed already\n",
            "931_simplex%1000%15.csv  existed already\n",
            "931_simplex%1000%25.csv  existed already\n",
            "931_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "7\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "1064_hypercube%1000%3.csv  existed already\n",
            "1064_hypercube%1000%15.csv  existed already\n",
            "1064_hypercube%1000%25.csv  existed already\n",
            "1064_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "1064_simplex%1000%3.csv  existed already\n",
            "1064_simplex%1000%15.csv  existed already\n",
            "1064_simplex%1000%25.csv  existed already\n",
            "1064_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "8\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "1197_hypercube%1000%3.csv  existed already\n",
            "1197_hypercube%1000%15.csv  existed already\n",
            "1197_hypercube%1000%25.csv  existed already\n",
            "1197_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "1197_simplex%1000%3.csv  existed already\n",
            "1197_simplex%1000%15.csv  existed already\n",
            "1197_simplex%1000%25.csv  existed already\n",
            "1197_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "9\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "1330_hypercube%1000%3.csv  existed already\n",
            "1330_hypercube%1000%15.csv  existed already\n",
            "1330_hypercube%1000%25.csv  existed already\n",
            "1330_hypercube%1000%50.csv  existed already\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "1330_simplex%1000%3.csv  existed already\n",
            "1330_simplex%1000%15.csv  existed already\n",
            "1330_simplex%1000%25.csv  existed already\n",
            "1330_simplex%1000%50.csv  existed already\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "10\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}